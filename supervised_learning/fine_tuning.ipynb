{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNPxOC8IBLvr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgCoqwfyD8aA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install fsspec==2023.6.0\n",
        "!python -m pip install seqeval==1.2.2\n",
        "!pip install git+https://github.com/huggingface/transformers@4.54.0.dev0\n",
        "!python -m pip install matplotlib==3.10.0 ipywidgets==7.7.1\n",
        "!pip install iterative-stratification==0.1.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IvMoJ2CEGNQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import datasets\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import transformers\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "from transformers import set_seed\n",
        "set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRThiwkdEtgb"
      },
      "outputs": [],
      "source": [
        "# https://huggingface.co/blog/modernbert\n",
        "\n",
        "model_id = \"answerdotai/ModernBERT-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ePQb4gvE7Uj"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"<Project Folder>\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_dataset = Dataset.from_json(os.path.join(BASE_DIR, \"data/dataset.jsonl\"))"
      ],
      "metadata": {
        "id": "z9odYVw3uX2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8LT3tpLFMHI"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "for i in range(len(hf_dataset)):\n",
        "  labels.extend(hf_dataset[i][\"str_label\"])\n",
        "\n",
        "labels = list(set(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQq3wDliFOLW"
      },
      "outputs": [],
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj0ZpnPVFRsS"
      },
      "outputs": [],
      "source": [
        "id2label = {idx:label for idx, label in enumerate(mlb.classes_)}\n",
        "label2id = {label:idx for idx, label in enumerate(mlb.classes_)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kfold_datasets = []\n",
        "for i in range(5):\n",
        "  kfold_datasets.append(DatasetDict.load_from_disk(os.path.join(BASE_DIR, \"data/k_fold_ds\", f\"{i}-fold\")))"
      ],
      "metadata": {
        "id": "53RhqAmBI9Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j22vlw-sFcPH"
      },
      "outputs": [],
      "source": [
        "def transform(example):\n",
        "  labels = mlb.transform([example[\"str_label\"]])[0]\n",
        "  return {\"label\": [float(label) for label in labels]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T5M7wpNFics"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch, text_field=\"text\"):\n",
        "  return tokenizer(batch[text_field], padding=\"longest\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kfold_datasets = [d.map(transform) for d in kfold_datasets]\n",
        "kfold_datasets = [d.map(tokenize, batched=True, batch_size=None, fn_kwargs={\"text_field\": \"text\"}) for d in kfold_datasets]"
      ],
      "metadata": {
        "id": "whayLs9PZhOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49LhXm9TF0BL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_id,\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           num_labels=len(labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "def create_timestamped_folder(base_directory, timestamp, purpose_token=\"training_run\"):\n",
        "    folder_name = f\"{purpose_token}_{timestamp}\"\n",
        "    folder_path = os.path.join(base_directory, folder_name)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        print(f\"Successfully created folder: {folder_path}\")\n",
        "        return folder_path\n",
        "    except OSError as e:\n",
        "        print(f\"Error creating directory {folder_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "5ubfk0aCQcdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "output_folder = os.path.join(BASE_DIR, \"output\")\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    print(f\"Output folder created at: {output_folder}\")\n",
        "\n",
        "trainer_checkpoints = create_timestamped_folder(output_folder, timestamp, \"trainer_checkpoints\")\n",
        "if trainer_checkpoints:\n",
        "    print(f\"Trainer Checkpoints created at: {trainer_checkpoints}\")\n",
        "\n",
        "kfold_best_checkpoints = create_timestamped_folder(output_folder, timestamp, \"kfold_checkpoints\")\n",
        "if kfold_best_checkpoints:\n",
        "    print(f\"Default folder created at: {kfold_best_checkpoints}\")"
      ],
      "metadata": {
        "id": "r_lmE8emQquN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qztw032tF2uK"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6K9K0bnFt6n"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "gradient_accumulation_steps = 8\n",
        "num_epochs = 50\n",
        "metric_name_for_early_stopping = \"eval_loss\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jf2_BidF62N"
      },
      "outputs": [],
      "source": [
        "for idx, d in enumerate(kfold_datasets):\n",
        "  logging_steps = len(d[\"train\"]) // batch_size\n",
        "\n",
        "  args = TrainingArguments(\n",
        "    output_dir=trainer_checkpoints,\n",
        "\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "\n",
        "    save_total_limit = 2,\n",
        "    save_strategy = \"epoch\",\n",
        "\n",
        "    metric_for_best_model=metric_name_for_early_stopping,\n",
        "    greater_is_better = False,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    eval_strategy = \"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    log_level=\"error\",\n",
        "\n",
        "    disable_tqdm=False,\n",
        "    report_to=\"none\"\n",
        "  )\n",
        "\n",
        "  early_stopping_callback = EarlyStoppingCallback(\n",
        "      early_stopping_patience=5,    # wait 5 epochs for improvement\n",
        "      early_stopping_threshold=0.001 # loss must decrease by at least 0.001 to count as an improvement\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model_init=model_init,\n",
        "      args = args,\n",
        "      train_dataset=d[\"train\"],\n",
        "      eval_dataset=d[\"valid\"],\n",
        "      tokenizer=tokenizer,\n",
        "      callbacks=[early_stopping_callback]\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  with open(os.path.join(kfold_best_checkpoints, f\"fold{idx}_training_history.pkl\"), 'wb') as f:\n",
        "    pickle.dump(trainer.state.log_history, f)\n",
        "\n",
        "  best_ckpt_path = trainer.state.best_model_checkpoint\n",
        "  trainer.save_model(os.path.join(kfold_best_checkpoints, f\"fold-{idx}\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}