{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cij2geurDWoV"
      },
      "source": [
        "## Connecting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj3UZ_SKunUG",
        "outputId": "68dc0a59-244c-4594-cdea-8aed3c424369"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0HGvgnDbPi"
      },
      "source": [
        "## Installing all the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9s3ApqYu_mN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index\n",
        "!pip install openai\n",
        "!pip install pypdf\n",
        "!pip install --upgrade llama_index\n",
        "\n",
        "# to use llama-index embeddings\n",
        "!pip install llama-index-embeddings-openai\n",
        "\n",
        "# to use arabert as the embedding model\n",
        "# !pip install arabert\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "# !pip install llama-index-llms-huggingface\n",
        "!pip install transformers torch\n",
        "\n",
        "# !pip install llama_index.core.node_parser\n",
        "# %pip install jiwer gradio typing-extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFPKfYCZDOrv"
      },
      "source": [
        "## Data and Persist Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdOm4JLBIMbu"
      },
      "outputs": [],
      "source": [
        "data_folder = \"your data folder\"\n",
        "PERSIST_DIR = \"your vector store index folder\"\n",
        "\n",
        "QA_CSV = \"CSV for QA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wYmQ1JIDXp"
      },
      "source": [
        "## Setting up the OPENAI API for Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIB0KSmBIDXs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# setting up the API key to use OpenAI API\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"  # replace with your OpenAI API key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUPcbbk7DDtY"
      },
      "source": [
        "## Getting all the files in the data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNhXq8-7HJ-k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def get_files(root_folder):\n",
        "    files = []\n",
        "    for foldername, subfolders, filenames in os.walk(root_folder):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(('.pdf', '.csv','.txt')):\n",
        "                # not a generic case\n",
        "                if '_' not in filename:\n",
        "                    files.append(os.path.join(foldername, filename))\n",
        "    return files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTi-JEsll6a3"
      },
      "outputs": [],
      "source": [
        "def read_file(file_path):\n",
        "    file = open(file_path,'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhJLxRWqMsnl"
      },
      "source": [
        "## Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25AbauXzGtjR"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response.pprint_utils import *\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load aragpt2 model directly\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "def get_retriever(PERSIST_DIR, top_k = 20):\n",
        "    index = None\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading index from storage: {PERSIST_DIR}\")\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "        index = load_index_from_storage(storage_context=storage_context)\n",
        "        print(\"Index loaded successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Caught an exception: {e}\")\n",
        "\n",
        "    if index is None:\n",
        "        raise ValueError(\"Failed to load or create the index.\")\n",
        "\n",
        "    retriever = VectorIndexRetriever(index=index, similarity_top_k=top_k)\n",
        "    print(f\"--- Successfully created the Retriever from {PERSIST_DIR} ---\")\n",
        "    return retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOwA0imDl6a9"
      },
      "source": [
        "## Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfcwMdX-6qKF"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(context, question):\n",
        "    prompt = f'''Answer the question using the following description of the information flow from Intelligent Transport System (ITS).\n",
        "            Choose your answer carefully analyzing the context entirely.\n",
        "            -----------------------------------------\n",
        "            {context}\n",
        "            -----------------------------------------\n",
        "            Question: {question}\n",
        "            -----------------------------------------\n",
        "            Your answer should be a python list:\n",
        "\n",
        "    '''\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part is helpful to count the length of your prompt. You may adjust the parameters of the RAG model based on the token size of your prompt. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XI-PSLll6a-"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import openai\n",
        "\n",
        "def count_tokens(text):\n",
        "\n",
        "    # Initialize the encoder for the specific model\n",
        "    encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Encode the prompt to get the token count\n",
        "    tokenized_prompt = encoder.encode(text)\n",
        "    token_count = len(tokenized_prompt)\n",
        "    return token_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVsGKAMkl6bD"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI7O-wEYl6bD"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI()\n",
        "def get_response(retriever, question, top_k =10, model = \"gpt-4o-mini-2024-07-18\"):\n",
        "\n",
        "    prompt = question\n",
        "    nodes = retriever.retrieve(question)\n",
        "\n",
        "    contexts = []\n",
        "    ind = 0\n",
        "    responses = []\n",
        "\n",
        "    sources = []\n",
        "    refs = []\n",
        "    i = 0\n",
        "    l = []\n",
        "    prompt = f'''You are given the description of some information flow from an Intelligent Transport System (ITS). Based on the description and the source and destination of the flow, you have to find out potential MITRE ATT&CK technique that the attacker might use to intervene the flow. Answer in the required format.\n",
        "                -------------------------------------------------------------\n",
        "                Description and Context:\\n\n",
        "                \\n\n",
        "                -------------------------------------------------------------\n",
        "                Question:\n",
        "                '''\n",
        "    for node in nodes:\n",
        "        if ind == len(contexts):\n",
        "            contexts.append(\"\")\n",
        "\n",
        "        text = '----------\\n'\n",
        "        text += 'Description:\\n'\n",
        "        text += node.node.text\n",
        "        text += 'Source: '\n",
        "        text += ' (' + node.node.metadata['file_path'].split('/')[-1][:-4] + ')' + \"\\n\\n\"\n",
        "        text += '----------\\n'\n",
        "\n",
        "        if count_tokens(contexts[ind] + text + question) + count_tokens(prompt) + 100 + 2000>= 16385:\n",
        "            ind+=1\n",
        "            continue\n",
        "\n",
        "        contexts[ind] += text\n",
        "\n",
        "        i += 1\n",
        "        if i==top_k:\n",
        "            break\n",
        "\n",
        "    ret = \"\"\n",
        "    for j,context in enumerate(contexts):\n",
        "        prompt = generate_prompt(context, question)\n",
        "\n",
        "        gpt_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages= [\n",
        "            {\n",
        "                \"role\":\"system\",\n",
        "                \"content\":'''You are a helpful assisstant. Your name is Transportation Security AI. Your role is to help with transportation security.\n",
        "                There are some instructions in each prompt. Follow those instructions strictly.'''\n",
        "            },\n",
        "            {\n",
        "                \"role\":\"user\",\n",
        "                \"content\":prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=10000,\n",
        "        top_p=0,\n",
        "        )\n",
        "        ret += str(gpt_response.choices[0].message.content)\n",
        "\n",
        "    return ret\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l9QMWNpKlPD"
      },
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn9iMxPIKlPE"
      },
      "source": [
        "### OPENAI Embedding Retriever + OPENAI Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNKdlKNrKlPG"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXHrT4HDpLL_",
        "outputId": "8fa5f08d-b7ce-49fb-aabb-15d7183b27e4"
      },
      "outputs": [],
      "source": [
        "retriever = get_retriever(PERSIST_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0PtrpWdquEC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def parse_mitre_techniques(response):\n",
        "    \"\"\"\n",
        "    Extracts MITRE Technique IDs from a response string and returns them as a Python list.\n",
        "\n",
        "    Args:\n",
        "        response (str): The response string containing MITRE Technique IDs.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of extracted MITRE Technique IDs.\n",
        "    \"\"\"\n",
        "    # Define a regular expression to match MITRE Technique IDs (e.g., \"T1234\")\n",
        "    mitre_pattern = r'T\\d{4}'\n",
        "\n",
        "    # Find all matches in the response string\n",
        "    mitre_techniques = re.findall(mitre_pattern, response)\n",
        "\n",
        "    return mitre_techniques\n",
        "\n",
        "\n",
        "def calculate_metrics(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculates TP, FP, FN, Precision, Recall, and F1-score for a single prediction.\n",
        "\n",
        "    Args:\n",
        "        true_labels (list): Ground truth technique IDs.\n",
        "        predicted_labels (list): Predicted technique IDs.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with TP, FP, FN, precision, recall, and F1-score.\n",
        "    \"\"\"\n",
        "\n",
        "    # print(true_labels)\n",
        "\n",
        "    true_set = set(true_labels)\n",
        "    predicted_set = set(predicted_labels)\n",
        "\n",
        "    # Calculate TP, FP, and FN\n",
        "    tp = len(true_set & predicted_set)\n",
        "    fp = len(predicted_set - true_set)\n",
        "    fn = len(true_set - predicted_set)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"tp\": tp,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmyQTWVFqDPr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = 'data folder location'  # Update this if the file is in a different location\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Initialize variables\n",
        "ground_truth = df['str_label']\n",
        "\n",
        "prompts = df['TEXT-flow_fn_process_threat'].tolist() # rename with your column name\n",
        "responses = []\n",
        "\n",
        "# client = openai.OpenAI()\n",
        "# Function to query OpenAI GPT model\n",
        "\n",
        "for prompt in prompts:\n",
        "\n",
        "    final_prompt =  f'''I am trying to do a multilabel classification of information flow description from Intelligent Transportation System (ITS) to MITRE ATT&CK Techniques.\n",
        "                Here we have information flow name, its source and destination, some functional object description associated with it and the description of the information flow itself. It also has a threat report generated in the STRIDE framework.\n",
        "                An attacker may attempt to compromise the integrity, confidentiality, or availability of the information flow in many ways.\n",
        "                Find all the relevant MITRE ATT&CK techniques that the attacker might use to attack the information flow.\n",
        "\n",
        "                Follow the instructions below carefully.\n",
        "\n",
        "                1. We have a predefined list of MITRE ATT&CK Techniques that consists of 63 MITRE Techniques. You have to choose only the relevant MITRE ATT&CK Techniques from this list that is relevant to the information flow given.\n",
        "\n",
        "                2. Understand the entire context, then generate a sublist of MITRE ATT&CK Technique from the given list.\n",
        "\n",
        "                3. Do not add any other description in your answer.\n",
        "\n",
        "                4. Only return the Technique IDs in python list format\n",
        "\n",
        "                Given MITRE Technique List = ['T1495','T1485','T1595','T1134','T1040','T1132','T1098','T1069','T1036','T1562','T1187','T1486','T1119','T1027','T1498','T1654','T1548','T1082','T1552','T1614','T1531','T1204','T1529','T1046','T1489','T1195','T1566','T1659','T1059','T1213','T1133','T1080','T1005','T1078','T1001','T1190','T1203','T1136','T1491','T1033','T1189','T1068','T1652','T1049','T1020','T1041','T1021','T1105','T1518','T1200','T1053','T1557','T1056','T1087','T1565','T1499','T1657','T1559','T1074','T1106','T1560', 'T1556', 'T1589']\n",
        "\n",
        "                Here is the information flow description:\n",
        "                {prompt}\n",
        "\n",
        "                Which are the relevant MITRE ATT&CK Techniques from the given list that the attacker might use to attack the information flow? Return the Technique IDs in python list format.\n",
        "                '''\n",
        "\n",
        "    response = get_response(retriever, final_prompt)\n",
        "    list_response = parse_mitre_techniques(response)\n",
        "    # print(list_response)\n",
        "    responses.append(list_response)\n",
        "\n",
        "# Compare responses to ground truth\n",
        "true_positives, false_positives, false_negatives = 0, 0, 0\n",
        "\n",
        "# Initialize results\n",
        "predictions_with_metrics = []\n",
        "\n",
        "for truth, pred in zip(ground_truth, responses):\n",
        "    p_truth = re.findall(r\"T\\d+\", truth)\n",
        "    # print(p_truth)\n",
        "    metrics = calculate_metrics(p_truth, pred)\n",
        "\n",
        "    predictions_with_metrics.append({\n",
        "        \"true_label\": p_truth,\n",
        "        \"predicted_label\": pred,\n",
        "        **metrics\n",
        "    })\n",
        "\n",
        "\n",
        "# Compute overall metrics\n",
        "overall_tp = sum(row[\"tp\"] for row in predictions_with_metrics)\n",
        "overall_fp = sum(row[\"fp\"] for row in predictions_with_metrics)\n",
        "overall_fn = sum(row[\"fn\"] for row in predictions_with_metrics)\n",
        "\n",
        "overall_precision = overall_tp / (overall_tp + overall_fp) if (overall_tp + overall_fp) > 0 else 0\n",
        "overall_recall = overall_tp / (overall_tp + overall_fn) if (overall_tp + overall_fn) > 0 else 0\n",
        "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uFQT2X6qlI4",
        "outputId": "77d9a86e-0afa-4a45-ef75-65484b005c43"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save predictions with metrics\n",
        "predictions_with_metrics_path = data_folder + \"filename_to_save_the_predictions.json\"\n",
        "with open(predictions_with_metrics_path, \"w\") as f:\n",
        "    json.dump(predictions_with_metrics, f, indent=4)\n",
        "\n",
        "# Save overall metrics\n",
        "overall_metrics_path = data_folder + \"filename_to_save_the_overall_prediction.json\"\n",
        "with open(overall_metrics_path, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"overall_precision\": overall_precision,\n",
        "        \"overall_recall\": overall_recall,\n",
        "        \"overall_f1\": overall_f1\n",
        "    }, f, indent=4)\n",
        "\n",
        "print(f\"Predictions with metrics saved to {predictions_with_metrics_path}\")\n",
        "print(f\"Overall metrics saved to {overall_metrics_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
