{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq_MXg3EG2yW"
      },
      "source": [
        "# Vector Store Index\n",
        "This code will create embeddings of the files and store them in a vector stroe index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cij2geurDWoV"
      },
      "source": [
        "## Connecting to Google Drive\n",
        "Google drive is required to connect if you want to store the index in google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj3UZ_SKunUG",
        "outputId": "37bc205d-6305-4c00-8f1e-be2bf7e3accb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0HGvgnDbPi"
      },
      "source": [
        "## Installing all the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ment0eGXrYoE",
        "outputId": "cb536fbb-97c0-474b-c8de-a7faeb27ad64"
      },
      "outputs": [],
      "source": [
        "!pip show llama-index\n",
        "!pip install llama-index --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9s3ApqYu_mN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-index\n",
        "!pip install openai\n",
        "!pip install pypdf\n",
        "!pip install --upgrade llama_index\n",
        "\n",
        "# to use llama-index embeddings\n",
        "!pip install llama-index-embeddings-openai\n",
        "\n",
        "# to use arabert as the embedding model\n",
        "!pip install arabert\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install transformers torch\n",
        "\n",
        "!pip install llama_index.core.node_parser\n",
        "!pip install jiwer gradio typing-extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFPKfYCZDOrv"
      },
      "source": [
        "## Data and Persist Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQLHjzvHGSOS"
      },
      "outputs": [],
      "source": [
        "## -- Transit Security Project -- ##\n",
        "\n",
        "data_folder = \"your data folder\" # this folder will have the data that you want\n",
        "                                # to embed and store in the Vector index\n",
        "PERSIST_DIR = \"your vector store index folder\" # this is the location of the vector store index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wYmQ1JIDXp"
      },
      "source": [
        "## Setting up the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIB0KSmBIDXs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# setting up the API key to use OpenAI API\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"  # replace with your OpenAI API key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWMrEOkQOs1v"
      },
      "source": [
        "## Setting up the Embedding model\n",
        " By default, LlamaIndex uses text-embedding-ada-002 from OpenAI. We also support any embedding model offered by Langchain here, as well as providing an easy to extend base class for implementing your own embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7gVNvhYH_gj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "\n",
        "# You can change the chunk size and overlap as your need\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUPcbbk7DDtY"
      },
      "source": [
        "## Getting all the files in the data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNhXq8-7HJ-k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def get_files(root_folder):\n",
        "    files = []\n",
        "    # List of labels to match against filenames\n",
        "    # Our dataset had only 61 labels, but you can add more as needed\n",
        "    labels = ['T1495','T1485','T1595','T1134','T1040','T1132','T1098','T1069','T1036','T1562','T1187','T1486','T1119','T1027','T1498','T1654','T1548','T1082','T1552','T1614','T1531','T1204','T1529','T1046','T1489','T1195','T1566','T1659','T1059','T1213','T1133','T1080','T1005','T1078','T1001','T1190','T1203','T1136','T1491','T1033','T1189','T1068','T1652','T1049','T1020','T1041','T1021','T1105','T1518','T1200','T1053','T1557','T1056','T1087','T1565','T1499','T1657','T1559','T1074','T1106','T1560', 'T1589']\n",
        "\n",
        "    for foldername, subfolders, filenames in os.walk(root_folder):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(('.pdf', '.csv','.txt')):\n",
        "                # not a generic case\n",
        "                if '_' not in filename:\n",
        "                    print(filename)\n",
        "                    if filename[:-4] in labels:\n",
        "                        files.append(os.path.join(foldername, filename))\n",
        "    return files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhJLxRWqMsnl"
      },
      "source": [
        "## Creating and Appending the index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvZHCMqjIQrO"
      },
      "source": [
        " ### Creating the embeddings, index and retriever using Vector Store Index\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25AbauXzGtjR",
        "outputId": "c66930bd-a957-426b-8825-d22e454c3e0e"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response.pprint_utils import *\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load aragpt2 model directly\n",
        "# from transformers import AutoModelForCausalLM\n",
        "\n",
        "import psutil\n",
        "\n",
        "# from llama_index.core.node_parser import SimpleNodeParser\n",
        "\n",
        "DATA_FOLDER = data_folder\n",
        "BATCH_SIZE = 200  # Define the batch size\n",
        "\n",
        "# Function to print memory usage\n",
        "def print_memory_usage():\n",
        "    # Print the current memory usage\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    print(f\"Memory Usage: {memory_info.percent}%\")\n",
        "\n",
        "index = None\n",
        "\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(PERSIST_DIR):\n",
        "        # creating the index from the documents\n",
        "        print(f\"Creating directory: {PERSIST_DIR}\")\n",
        "        os.mkdir(PERSIST_DIR)\n",
        "\n",
        "        files = get_files(DATA_FOLDER)\n",
        "        print(len(files))\n",
        "        if not files:\n",
        "            raise ValueError(\"No files found in the specified data folder.\")\n",
        "\n",
        "        # Process files in batches\n",
        "        for i in range(0, len(files), BATCH_SIZE):\n",
        "            batch_files = files[i:i + BATCH_SIZE]\n",
        "            # print(f\"Loading documents from files: {batch_files}\")\n",
        "            documents = SimpleDirectoryReader(input_files=batch_files).load_data()\n",
        "            # print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "            if index is None:\n",
        "                print(\"Creating VectorStoreIndex...\")\n",
        "                index = VectorStoreIndex.from_documents(documents=documents)\n",
        "                print(f\"--- Files {i} to {i + BATCH_SIZE-1} (Ceated new Index) ---\")\n",
        "            else:\n",
        "                # index.add_documents(documents=documents) ##\n",
        "                for document in documents:\n",
        "                    index.insert(document=document)\n",
        "                # # Parse documents into nodes\n",
        "                # parser = SimpleNodeParser()\n",
        "                # new_nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "                # # Add nodes to the existing index\n",
        "                # index.insert_nodes(new_nodes)\n",
        "                print(f\"--- Files {i} to {i + BATCH_SIZE-1} (added to the existing Index) ---\")\n",
        "\n",
        "            # Persist the index after each batch\n",
        "            print(f\"Persisting index to {PERSIST_DIR}...\")\n",
        "            index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "            print(\"Index persisted successfully.\")\n",
        "\n",
        "            # Print memory usage\n",
        "            print_memory_usage()\n",
        "            # break\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Retrieving a storage context from already existing context and loading the index\n",
        "        print(f\"Loading index from storage: {PERSIST_DIR}...\")\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "        index = load_index_from_storage(storage_context=storage_context)\n",
        "        print(\"Index loaded successfully.\")\n",
        "\n",
        "    # Check if index is loaded successfully\n",
        "    if index is None:\n",
        "        raise ValueError(\"Failed to load or create the index.\")\n",
        "\n",
        "    retriever = VectorIndexRetriever(index=index, similarity_top_k=20)\n",
        "    postprocessor = SimilarityPostprocessor(similarity_cutoff=0.60)\n",
        "    print(\"--- Successfully created the embeddings, index, and retriever ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Caught an exception: {e}\")\n",
        "    if os.path.exists(PERSIST_DIR):\n",
        "        os.rmdir(PERSIST_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0iPgJhPAINX"
      },
      "source": [
        "### This code is to append the index\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZHj8uKfAOfA",
        "outputId": "5d438c7c-7d88-4678-8f60-219d37b15660"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response.pprint_utils import *\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load aragpt2 model directly\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "import psutil\n",
        "\n",
        "# from llama_index.core.node_parser import SimpleNodeParser\n",
        "\n",
        "DATA_FOLDER = data_folder\n",
        "BATCH_SIZE = 1  # Define the batch size\n",
        "starting_index = 8 # file index from where the vector index starts being appended\n",
        "\n",
        "# Function to print memory usage\n",
        "def print_memory_usage():\n",
        "    # Print the current memory usage\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    print(f\"Memory Usage: {memory_info.percent}%\")\n",
        "\n",
        "index = None\n",
        "\n",
        "try:\n",
        "    if os.path.exists(PERSIST_DIR):\n",
        "        print(f\"Loading index from storage: {PERSIST_DIR}...\")\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "        index = load_index_from_storage(storage_context=storage_context)\n",
        "        print(\"Index loaded successfully.\")\n",
        "\n",
        "        files = get_files(DATA_FOLDER)\n",
        "        if not files:\n",
        "            raise ValueError(\"No files found in the specified data folder.\")\n",
        "\n",
        "        # Process files in batches\n",
        "        for i in range(starting_index, 660, BATCH_SIZE):\n",
        "            batch_files = files[i:i + BATCH_SIZE]\n",
        "            # print(f\"Loading documents from files: {batch_files}\")\n",
        "            documents = SimpleDirectoryReader(input_files=batch_files).load_data()\n",
        "            # print(f\"Loaded {len(documents)} documents.\")\n",
        "\n",
        "            if index is None:\n",
        "                print(\"Creating VectorStoreIndex...\")\n",
        "                index = VectorStoreIndex.from_documents(documents=documents)\n",
        "                print(f\"--- Files {i} to {i + BATCH_SIZE-1} (Ceated new Index) ---\")\n",
        "\n",
        "            else:\n",
        "                # index.add_documents(documents=documents) ##\n",
        "                for document in documents:\n",
        "                    index.insert(document=document)\n",
        "\n",
        "                print(f\"--- Files {i} to {i + BATCH_SIZE-1} (added to the existing Index) ---\")\n",
        "\n",
        "            # Persist the index after each batch\n",
        "            print(f\"Persisting index to {PERSIST_DIR}...\")\n",
        "            index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "            print(\"Index persisted successfully.\")\n",
        "\n",
        "            # Print memory usage\n",
        "            print_memory_usage()\n",
        "\n",
        "    else:\n",
        "        # Retrieving a storage context from already existing context and loading the index\n",
        "        print(\"PERSIST dir does not exist. Create from the start\")\n",
        "\n",
        "\n",
        "    # Check if index is loaded successfully\n",
        "    if index is None:\n",
        "        raise ValueError(\"Failed to load or create the index.\")\n",
        "\n",
        "    retriever = VectorIndexRetriever(index=index, similarity_top_k=20)\n",
        "    postprocessor = SimilarityPostprocessor(similarity_cutoff=0.60)\n",
        "    print(\"--- Successfully created the embeddings, index, and retriever ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Caught an exception: {e}\")\n",
        "    # if os.path.exists(PERSIST_DIR):\n",
        "        # os.rmdir(PERSIST_DIR)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
